---
title: "Day 1: Basic data manipulation and visualization"
author: "Nayef Ahmad - VCH Decision Support"
date: "September 14, 2018"
output: 
    html_document: 
        toc: yes
        code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)

library("kableExtra")


# shortcuts: 
# > ctrl + alt + i ==> new code chunk
# > ctrl + shift + k ==> knit doc 
# \  ==> line break (include 2 spaces after)
```

# Part 1 

## Why are we talking about R/Python? 

#### They are the industry standard...

* [The impressive growth of R](https://stackoverflow.blog/2017/10/10/impressive-growth-r/)
* [The incredible growth of Python](https://stackoverflow.blog/2017/10/10/impressive-growth-r/)
* R is particularly widely used in [healthcare](https://nhsrcommunity.com/about/) 
* It's in [high demand in the data science job market](https://www.cio.com/article/3263790/data-science/the-essential-skills-and-traits-of-an-expert-data-scientist.html) 

#### ...for good reason 

* R is free and open-source
* Very active user and contributor base 
* [R packages allow users to employ cutting-edge statistics, econometrics, optimization, machine learning and simulation techniques. This makes R the leading analytics language in academia and industry](https://www.coursera.org/lecture/decision-making/the-role-of-r-K57sl)

\  

Ok, let's get started 

*********


## Set up your project
1. Create a folder where you'll save project files. I recommend following a folder structure similar to the one mentioned [here](https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/) 
2. Open RStudio, create a new project, save it in the root folder. 
3. Open a new R script. Now let's get started with the code! 

\  
\  
\  

## Install the packages you need
You will only need to do this once. I recommend running these lines in the console instead of the script editor. 

```{r  }
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("here")
# install.packages("readr")
# install.packages("GGally")
# install.packages("gapminder")

```

## Load the packages for the current session
```{r message = FALSE}
library("dplyr")  # for manipulating data frames 
library("ggplot2")  # for data viz
library("here")  # for simplifying folder references 
library("readr")  # reading and writing data files 
library("GGally")  # extension to ggplot2
library("gapminder")  # gapminder dataset
library("janitor")  # for cleaning up dataframes 

```

*******************

\  
\  
\  

## Built-in datasets
R comes with some built-in datasets. Let's start by taking a look at a few of them. 

### mtcars dataset 
```{r}
mtcars %>% 
    
    # don't use the following lines; they're specifically for Rmarkdown files
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

```

### Summary functions: 
These functions are good for getting a quick sense of a dataset. 

```{r}
str(mtcars)
```

```{r}
summary(mtcars)
```

```{r}
head(mtcars)  # by default shows top 10 rows 
```

\  

### Mini-exercise 1
Try doing the same with the following datasets: 

* iris
* airquality
* diamonds

******************************

\  
\  
\  

## Outputting data from R 
We're going to save mtcars as a csv file somewhere in our project folder. Note that with the here( ) package, we don't have to specify a full file path - just a path relative to the project root. This means you can copy the whole project to another location on your computer, or a completely different computer, and all file references should still work. 

```{r}
# Where is the root folder right now? 
here()  # result will be different for everyone 

```

```{r}
# save to the sub-folder "results" >> "output from src"
write_csv(cbind(rownames(mtcars), mtcars),    # save WHAT?
          here("results",                     # save WHERE?    
               "output from src",      
               "mtcars-original.csv"))

```

**************************

\  
\  
\  

## Side-by-side analysis in R and Excel
Take at look at [the basic data manipulation vocabulary](https://dplyr.tidyverse.org/)

We're going to do the same steps in R and Excel. Note that once you do the steps in R, you will never have to do them again - your work is "conserved". When the data changes, you don't have to do the work all over again, just run the code, and it will repeat all the steps you specified. The same is not true in Excel. 

This may seem trivial when you're thinking of doing analysis one file at a time in a "manual" fashion. However, to multiply the productivity and effectiveness of a data science team, it is necessary to automate these tasks so that they can be performed on dozens or even hundreds of files simultaneously without human intervention. 

```{r}
df1.mtcars.modified <- mtcars %>% 
    # Notes on notation: 
    # "x <- 10" means "save the number 10 with object name x"
    # "%>%" translates as "then". That is, first do x %>% do y
    
    # select certain COLUMNS
    select(cyl, 
           mpg, 
           disp) %>% 
    
    # filter out certain ROWS
    filter(mpg <= 30) %>%  # let's say these are outliers 
    
    rename(cylinders = cyl, 
           miles.per.gallon = mpg, 
           displacement = disp) %>% 
    
    # let's say one of the entries of mpg was a known data error: 
    mutate(miles.per.gallon = case_when(
        miles.per.gallon == 15 ~ 15.5,  # "~" is like the "then" statement in SQL
        TRUE ~ miles.per.gallon
        )) %>% 
    
    # Wait, what kind of savages use units like miles and gallons? 
    # let's create a new column with proper civilized units: 
    mutate(kilometres.per.litre = (1.609*miles.per.gallon)/3.785) %>% 
    group_by(cylinders) %>% 
    
    summarise(avg.kpl = mean(kilometres.per.litre) %>% round(1), 
              avg.disp = mean(displacement) %>% round(1))
    

```

```{r}
# show the output
df1.mtcars.modified %>% 
    kable %>%  # use this to print 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
```

\  
\  
\  

## Output the modified dataset

```{r}
write_csv(df1.mtcars.modified, 
          here("results",
               "output from src",      
               "mtcars-summarized.csv"))

```



## Read in data: 
Although we already have mtcars built-in, let's practice reading in the csv file we created. 

```{r}
df2.mtcars.original <- read_csv(here("results",
                                     "output from src",
                                     "mtcars-original.csv"))
```


********************************

\  
\  
\  

## Data visualization with ggplot
### Scatterplots: 

```{r}
p1.overall.mean <- df2.mtcars.original %>% 
    
    # let's add in the kpl column again: 
    mutate(kpl = (1.609*mpg)/3.785) %>%
    
    # let's recode cyl as a discrete variable (aka "factor"): 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = kpl)) + 
    
    # geom_point creates a scatterpolot
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # overall mean: 
    stat_smooth(method = "lm", 
                formula = y ~ 1) + 
    
    theme_classic()
    
    
# print: 
p1.overall.mean
    


```

```{r}
p2.group.means <- df2.mtcars.original %>% 
    
    # let's add in the kpl column again: 
    mutate(kpl = (1.609*mpg)/3.785) %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = kpl)) + 
    
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # examine three different ways of summarizing behaviour within
    # each level of cyl: 
    
    # mean by group: 
    stat_smooth(aes(group = cyl,
                    colour = cyl), 
                method = "lm", 
                formula = y ~ 1) + 
    
    theme_classic()
    
# print: 
p2.group.means


```



```{r}
p3.group.trends <- df2.mtcars.original %>% 
    
    # let's add in the kpl column again: 
    mutate(kpl = (1.609*mpg)/3.785) %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = kpl)) + 
    
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # examine three different ways of summarizing behaviour within
    # each level of cyl: 
    
    # mean by group: 
    stat_smooth(aes(group = cyl,
                    colour = cyl), 
                method = "lm") + 
    
    theme_classic()
    
# print: 
p3.group.trends
```


```{r}
p4.overall.trend <- df2.mtcars.original %>% 
    
    # let's add in the kpl column again: 
    mutate(kpl = (1.609*mpg)/3.785) %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = kpl)) + 
    
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # examine three different ways of summarizing behaviour within
    # each level of cyl: 
    
    # mean by group: 
    stat_smooth() +  # also try "lm"
    
    theme_classic()
    
# print: 
p4.overall.trend
```

### Boxplot: 
```{r}
p5.box <- df2.mtcars.original %>% 
    
    # let's add in the kpl column again: 
    mutate(kpl = (1.609*mpg)/3.785) %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = cyl,  # specify x and y axis
               y = kpl)) + 
    
    geom_boxplot() + 
    
    # by default, boxplot shows only median, not mean
    # we'll add in the mean here: 
    stat_summary(fun.y = mean, 
                 geom = "point", 
                 colour = "firebrick") + 
    
    theme_classic()

# print: 
p5.box
```

### Summary of all important variables in the dataset
```{r}
p6.pairs <- df2.mtcars.original %>% 
    select(mpg, 
           cyl, 
           hp, 
           disp) %>% 
    
    mutate(cyl = as.factor(cyl)) %>% 
    
    ggpairs()
    
# print: 
p6.pairs

```



## Two ways of saving plots: 
### Individually with ggsave(): 
```{r message = FALSE}
ggsave(here("results", 
            "output from src", 
            "data-summary-plot.pdf"), 
       p6.pairs, 
       width = 10, 
       units = "in")
```


### Multiple plots using the pdf() device: 

```{r message = FALSE}
pdf(here("results", 
            "output from src", 
            "all-plots.pdf"), 
    width = 10)

p1.overall.mean
p2.group.means
p3.group.trends
p4.overall.trend
p5.box
p6.pairs
dev.off()


```

\  
\  
\  

## Let's talk about models for data 
Raw data is pretty much useless for decision-making in any complex environment. [Simply “presenting the data” is first, not really possible, and second, not desirable. It’s information overload and rarely allows the audience to make an intelligent conclusion.](https://simplystatistics.org/2018/09/14/divergent-and-convergent-phases-of-data-analysis/)

It is the job of the data analyst to convert the raw data into a form that is useful for decision-making. This means developing models that abstract away from the raw data by doing one of three things: 

1. Summarizing the data 
2. Making inferences from the data 
3. Making predictions from the data 

[This is a good summary of the difference between 2 and 3](https://www.coursera.org/lecture/managing-data-analysis/inference-vs-prediction-xKjFf)

We'll be covering several different statistical models in the next few days. For now, I encourage you to think about the models we have already developed here, and how you might develop them further. 

***********************

\   
\  
\  


## Visualizing the Gapminder dataset
```{r}
str(gapminder)
```

```{r}
p7.gapminder.pairs <- gapminder %>% 
    select(-c(country)) %>% 
    ggpairs()

p7.gapminder.pairs

```

### GDP per capita growth over time

```{r}
p8.gapminder.gdppercap <- gapminder %>% 
    ggplot(aes(x=year, 
               y=gdpPercap)) + 
    geom_jitter(aes(colour = continent) ,
                alpha = 0.2) + 
    stat_smooth(aes(group = continent, 
                    colour = continent)) + 
    # scale_y_log10() + 
    
    theme_classic()

p8.gapminder.gdppercap
```

How can we spread out the y-axis to see differences between the three lowest lines? 

```{r}
p9.gdppercap.log <- p8.gapminder.gdppercap + 
    scale_y_log10()

p9.gdppercap.log
```

### Relationship between life expectancy and gdp per capita 

```{r}
p10.life.gdp <- gapminder %>% 
    ggplot(aes(x = log(gdpPercap), 
               y = lifeExp)) + 
    
    geom_point(aes(colour = year), 
               alpha = 0.5) + 
    
    facet_wrap(~continent) + 
    
    geom_smooth(method = "lm")

p10.life.gdp
```

******************************************************

\  
\  
\  


# Part 2 

## Exercise: Exploring LGH ED data 

Use dplyr and gpplot2 (and any other packages you want to) to explore the LGH ED data saved in this shared folder: 

* *Folder link*: G:\VCHDecisionSupport\Learning\Peer Learning Sessions\2018-11-09_R for data analysis and modelling\data
* *Alternative* (for those on personal laptops): https://sftp.phsa.ca/?u=HdA4RmM&p=rLsWDWB 

Start by saving the data in the "data" sub-folder of your project. Then use the following command. 

```{r} 

# change the dataframe's name if you want to  
df3.ed.data <- read_csv(here::here("data", 
                             "ed-data.csv"), 
                        na = "NULL") %>% 
    
    # convert column names to lowercase, replace space with "_", etc.
    clean_names() %>%  # this is from the "janitor" package 
    
    # change data types: 
    mutate(ed_los_minutes = as.character(ed_los_minutes) %>% 
               as.integer, 
           ad_los_days = as.character(ad_los_days) %>% 
               as.integer, 
           age = as.character(age) %>% 
               as.integer)


```




### Questions for investigation: 

1) We are interested in predicting acute LOS in order to manage patient flow. Can we use ED LOS to predict Acute LOS? 

2) Can we "adjust for" CTAS? 

3) Relationship between age and acute LOS? 

4) Relationship between ED LOS and CTAS. 

5) Think about how quantifying these relationships can be useful for identifying anomalies, designing interventions, etc.  


Let's start by taking a quick look at some of the relationships between the variables. 


```{r}
str(df3.ed.data)

```



```{r }
p11.pairs <- df3.ed.data %>%
    select(ed_los_minutes, 
           ad_los_days, 
           age, 
           triage_acuity_code) %>% 
    
    ggpairs(); p11.pairs

# save output 
ggsave(here("results", 
            "output from src", 
            "01_ed-date.jpeg"))  # by default ggsave saves the last plot that was called 


```


### Two types of correlation: Pearson's and Spearman's 

Let's look at the correlation between ED LOS and acute LOS. Note that Pearson's correlation specifically measures the strength of *linear* relationships between 2 variables. 

On the other hand, Spearman's correlation is more general, and is able to quantify the strength of monotonic relationships. For more, see [here](https://stats.stackexchange.com/questions/8071/how-to-choose-between-pearson-and-spearman-correlation) 

```{r }
cor(df3.ed.data$ed_los_minutes, 
    df3.ed.data$ad_los_days, 
    method = "pearson",  # measures strength of *linear* relationship 
    use = "complete.obs")  # remove NA values 


```



```{r }
cor(df3.ed.data$ed_los_minutes, 
    df3.ed.data$ad_los_days, 
    method = "spearman",  # more general than pearson
    use = "complete.obs")

cor(df3.ed.data$age, 
    df3.ed.data$ad_los_days, 
    method = "spearman", 
    use = "complete.obs")  # 0.28


```


### Plotting Acute LOS vs ED LOS 

```{r }

p12.ed.and.ad.los <- df3.ed.data %>% 
    ggplot(aes(x = ed_los_minutes, 
               y = ad_los_days)) + 
    geom_point(alpha = 0.1) +  # alpha sets transparency of points 
    
    # data is too bunched together to see properly. 
    # let's convert to log-scales: 
    
    scale_y_log10(breaks = c(1, 5, 10, 30, 100)) + 
    scale_x_log10(breaks = c(30, 60, 600, 1000, 2000, 10000)) + 
    
    geom_smooth() +  # by default this uses method = GAM 
    
    geom_smooth(method = "lm", 
                colour = "skyblue", 
                se = FALSE) + 
    
    facet_wrap(~triage_acuity_code); p12.ed.and.ad.los

# save output 
ggsave(here("results", 
            "output from src", 
            "02_ed-data.jpeg"))  # by default ggsave saves the last plot that was called 



```


Note that for CTAS 2-4, the overall pattern is that as ED LOS increases, mean acute LOS also increases. This is not true for CTAS 1 patients. Also interesting is the "wall" at 600 minutes for CTAS 3 and CTAS 4 patients. The mean acute LOS actually seems to decrease slightly for patients who just cross the 600 min mark - it's like this is a completely separate patient population.  


\  
\  
\  

### Acute LOS vs ED LOS by age 

```{r}
p13.ed.and.ad.los.age.groups <- df3.ed.data %>%
    
    # remove CTAS levels with too few observations: 
    filter(!triage_acuity_code %in% c("5", NA)) %>% 
    
    mutate(age.group = case_when(
        age < 20 ~ "under 20 yr", 
        age >=20 & age <60 ~ "20-60 yr", 
        age >= 60 ~ "over 60 yr") %>% 
            factor(levels = c("under 20 yr", 
                              "20-60 yr", 
                              "over 60 yr"))) %>%
    
    ggplot(aes(x = ed_los_minutes, 
               y = ad_los_days)) + 
    geom_point(alpha = 0.1) + 
    geom_smooth(aes(group = age.group, 
                    colour = age.group), 
                se = TRUE, 
                alpha = 0.2) + 
    
    # zoom in on the x-axis: 
    # coord_cartesian(xlim = c(0, 600)) +
    scale_y_log10(breaks = c(1,5,10,30,100)) + 
    scale_x_log10(breaks = c(30, 60, 300, 600, 2000)) + 
        
    facet_wrap(~triage_acuity_code); p13.ed.and.ad.los.age.groups

# save output 
ggsave(here("results", 
            "output from src", 
            "03_ed-date.jpeg"))  # by default ggsave saves the last plot that was called 

```


### Acute LOS vs CTAS level: 

```{r}
p14.los.vs.ctas <- df3.ed.data %>% 
    
    mutate(age.group = case_when(
        age < 20 ~ 1, 
        age >=20 & age <60 ~ 2, 
        age >= 60 ~ 3) %>% 
            as.factor) %>% 
    
    # remove NA value of age
    filter(!is.na(age)) %>% 
    
    mutate(triage_acuity_code = as.factor(triage_acuity_code)) %>% 
    
    ggplot(aes(x = triage_acuity_code, 
               y = ad_los_days)) + 
    
    geom_boxplot() + 
    
    stat_summary(fun.y = mean, 
                 geom = "point", 
                 colour = "firebrick") + 
    
    # take logarithm of y-axis: 
    scale_y_log10(breaks = c(1, 2, 3, 4, 5, 10, 30, 60)) +  # by default, labels are in original units, not logged units
    
    # zoom in on y-axis: 
    # coord_cartesian(ylim = c(0.1, 60)) + 
    
    facet_wrap(~age.group); p14.los.vs.ctas



```

Note that this kind of plot helps us to determine what counts as an outlier, based on past data. E.g. for CTAS 5 patients under 20 years, acute LOS of over 5 days is an outlier, but for patients 20-60 years at CTAS 5, LOS would have to be over 30 days before it counted as an outlier. 

\  
\  
\  


# See you next time 
Well, that's the end of Day 1. Here's the link to the material we'll cover next time: https://rpubs.com/nayefahmad/day-2_time-series 





