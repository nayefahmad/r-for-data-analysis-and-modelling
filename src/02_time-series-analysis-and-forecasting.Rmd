---
title: 'Day 2: Time series analysis and forecasting'
author: "Nayef Ahmad - VCH Decision Support"
date: "November 3, 2018"
output: 
    html_document: 
      toc: yes
      code_folding: show 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)


library("kableExtra")
library(ggpubr)

# shortcuts: 
# > ctrl + alt + i ==> new code chunk
# > ctrl + shift + k ==> knit doc 
# \  ==> line break (include 2 spaces after)
```

## Load the packages and functions for the current session
We'll be using some forecasting-specific packages today, as well as the usual tidyverse ones. 
```{r message = FALSE, warning = FALSE}
library(dplyr)  # for manipulating data frames 
library(ggplot2)  # for data viz
library(here)  # for simplifying folder references 
library(readr)  # reading and writing data files 
library(fpp)  # forecasting principles and practices 
library(prophet)  # facebook's forecasting package 
library(janitor)  # for cleaning up dataframes  
library(lubridate)  # for working with dates 
library(tidyr)  # for tidying data
library(magrittr)  # for easy piping 

# import custom functions: 
# I will email these files to you; save them in your "src" 
#   folder
source(here::here("src", "stl.as.df_function.R"))  
source(here::here("src", "generate-time-series_function.R"))

```

# Part 1 
## Random generating processes 
Before we look at specific time series, let's take a look at some randomly generated ones. Note that there are no systematic factors underlying these series, but they can display characteristics similar to those we see in real time series. 

The point is, many of the patterns we think we see in time series are probably nothing but chance. If it is easy to generate a random dataset that looks very similar to your actual data, then maybe your actual data provides no evidence of any systematic factors?   

> Inexperienced researchers and laypeople alike usually overestimate the role of systematic factors relative to chance factors ^[Abelson, 1995, *Statistics as Principled Argument*, p7] 

```{r}
# try this to get lots of random walks: 
generate.ts_function(seed = FALSE)

```


```{r}
# try this to get lots of random walks: 
generate.ts_function(seed = FALSE)

```




## Features of time series data 

Take a look at the following 2 time series. How are they different? 

```{r}
# this won't work unless you have imported the function first
generate.ts_function()

```



We'll be examining three different components of a time series^[https://otexts.org/fpp2/tspatterns.html]: 

1) *Trend*: a long-term increase or decrease in the data. It does not have to be linear. 
2) *Seasonality*: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency.
3) *Smoothness/variability*: Are neighbouring values of the time series correlated with one another, or are they completely independent of one aonther? If yes, how strongly are they related? 

The series on the left has no trend, and very little variability. The one on the right has a significant positive trend, and is much "choppier", or less smooth. It may also have some seasonality, but we can't say for sure yet. 

Let's use some exploratory graphs to inspect the components of a time series. 



\  
\  
\  

## Visualizing time series data with the fpp package 
### Time series objects 
So far we have been working with dataframes. E.g. mtcars is a dataframe: 
```{r}
str(mtcars)
```

Most time series functions require data to be in a time series format. An example time series object is elecequip: 

```{r}
str(elecequip)

start(elecequip)
end(elecequip)
frequency(elecequip)  

```

Just to see how it works, let's convert one column of mtcars into a time series object. We use the ```ts``` function for that. 

```{r}
eg.time.seris <- mtcars %>% 
    select(mpg) %>% 
    ts()

str(eg.time.seris)
```

### The autoplot() function 
The ```autoplot``` knows how to deal with time series object, so that you don't have to specify all the individual components in ggplot. 

```{r}
autoplot(elecequip)
```


### The ggmonthplot() function 

```{r}
ggmonthplot(elecequip)
```


### The ggseasonplot() function 
```{r}
ggseasonplot(elecequip)
```

### The ggAcf() function 
The autocorrelation function (ACF) is useful for  
See here for more details on the autocorrelation function. 

```{r}
ggAcf(elecequip)

```

## Mini Exercise
Try creating the same graphs with these series: 

* ```usdeaths```
* ```fancy```
* ```dj```


## Time series decomposition 

```{r}
stl(elecequip, 
    s.window = "periodic") %>% plot

stl.as.df_fn(elecequip) %>% head

```


## Quick and dirty de-seasonalizing  
A quick and dirty method to deseasonalize a time series is to fit a robust linear model to the data, predicting the value of the series using the month as a predictor.^[see here for example: https://www.jstatsoft.org/article/view/v040i01] The residuals from this model are the deseasonalized series. 


```{r}
df1.elec <- data.frame(month = c(rep(month.abb, times = 15), 
                                 month.abb[1:11]),
                       year = c(rep(1996:2010, each = 12), 
                                rep(2011, 11)),
                       elecequip = as.numeric(elecequip)) %>% 
    mutate(timeperiod = 1:n())
                       
library(MASS)  # conflicts with dplyr select, so we didn't 
# load at the beginning 

m1.elec <- rlm(elecequip ~ month -1,  # "-1" removes intercept
               data = df1.elec)


unloadNamespace("MASS")  # unload the package 

# summary(m1.elec) 
df1.elec %<>% 
    mutate(deseasonalized = resid(m1.elec))

# plot deseasonalized series: 
p1.deseasonalized <- df1.elec %>% 
    ggplot(aes(x = timeperiod, 
               y = deseasonalized)) + 
    geom_line() + 
    labs(title = "Deseasonalised")


ggarrange(p1.deseasonalized, 
          autoplot(elecequip), 
          nrow = 2)




```





## Stationarity of time series  



 




\  
\  
\  
\  
\  


## Producing forecasts 

\  
\  
\  
\  
\  

## Evaluating forecasts 

\  
\  
\  
\  
\  



***********************************************************





***********************************************************

\  
\  
\  
\  
\  

# Part 2 
## Daily parking tickets in Vancouver
### Read in and clean data for 2016-17
```{r}
# if you want the entire parking data set, run this: 
# warning: this will take a long time. 

# df1.1.tickets.2017 <- read.csv(url("ftp://webftp.vancouver.ca/opendata/csv/2017Parking_Tickets.csv"))

# df1.2.tickets.2016 <- read.csv(url("ftp://webftp.vancouver.ca/opendata/csv/2016Parking_Tickets.csv"))

# in case it's easier to load this off disk instead of 
# over network: 
df1.1.tickets.2017 <- read_csv(here::here("data", 
                        "vancouver-parking-tickets-2017.csv")) %>%
    select(-"X1")

df1.2.tickets.2016 <- read_csv(here::here("data", 
                        "vancouver-parking-tickets-2016.csv"))

# union of the two datasets: 
df1.tickets <- rbind(df1.2.tickets.2016, 
                     df1.1.tickets.2017)


# let's do some wrangling: 
df1.tickets %<>% 
    clean_names() %>% 
    mutate(entry_date = mdy(entry_date)) %>% 
    mutate_if(is.character, 
              factor)

# view result: 
# str(df1.tickets)
# summary(df1.tickets)

```

Take a quick look at the data: 
```{r}
head(df1.tickets) %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

```



### Read in 2018 data (test set)
To understand whether our forecasting methodology is any good, we must compare its forecasts against data that it has never seen. We'll use 2018 data for this.  

```{r}
# df1.3.tickets.2018 <- read.csv(url("ftp://webftp.vancouver.ca/opendata/csv/2018Parking_Tickets.csv"))

# finally, read in 2018 data (test dataset)
df1.3.tickets.2018 <- read_csv(here::here("data", 
                        "vancouver-parking-tickets-2018.csv"))%>% clean_names() %>% 
    mutate(entry_date = mdy(entry_date)) %>% 
    mutate_if(is.character, 
              factor) 

# view result: 
head(df1.3.tickets.2018) %>% 
    union_all(tail(df1.3.tickets.2018)) %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")




```




### Group by date
Note that we have to join on list of all dates in order to make sure there are no missing dates in the data. In case you're keeping track, joins are something that you can do in Excel, but it's a bit of a pain. ^[https://superuser.com/questions/420635/how-do-i-join-two-worksheets-in-excel-as-i-would-in-sql]

```{r}
# group by 
df2.daily.tickets <- df1.tickets %>% 
    group_by(entry_date) %>% 
    summarise(count = n())

# all dates: 
df3.1.all.dates <- data.frame(dates = seq(as.Date("2016-01-01"),
                                        as.Date("2017-12-31"),
                                        by="1 day"))

# full join: 
df3.daily.tickets.joined <- df2.daily.tickets %>% 
    full_join(df3.1.all.dates, 
              by = c("entry_date" = "dates")) %>% 
    arrange(entry_date) %>% 
    
    # replace NAs with 0s: 
    mutate(count = replace_na(count, 0))


# view result: 
# str(df3.daily.tickets.joined)
# summary(df3.daily.tickets.joined)
head(df3.daily.tickets.joined) %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

    
tail(df3.daily.tickets.joined) %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")



```

Interesting that there's no data for Christmas and New Year's.  
Let's do the same grouping for the 2018 data.

```{r}
df4.daily.tickets.2018 <- 
    df1.3.tickets.2018 %>% 
    group_by(entry_date) %>% 
    summarise(count = n())

df4.1.all.dates <- data.frame(dates=seq(as.Date("2018-01-01"),
                                        as.Date("2018-12-31"),
                                        by="1 day"))

df4.daily.tickets.2018 %<>% 
    full_join(df4.1.all.dates, 
              by = c("entry_date" = "dates")) %>% 
    arrange(entry_date) 
    
    # replace NAs with 0s: 
    # mutate(count = replace_na(count, 0))

# str(df4.daily.tickets.2018)
# summary(df4.daily.tickets.2018)
# head(df4.daily.tickets.2018)
# tail(df4.daily.tickets.2018)

```


\  
\  
\  

### Visualize time series 
```{r}
p1.parking.2017 <- df2.daily.tickets %>% 
    ggplot(aes(x = entry_date, 
               y = count)) + 
    geom_line() + 
    labs(title = "Daily parking tickets in Vancouver, BC", 
         subtitle = "2016-01-01 to 2017-12-31\n\n", 
         caption = "\n\nSource:https://data.vancouver.ca/datacatalogue/parking-tickets.htm") + 
    
    geom_smooth() +
    
    theme_classic(base_size = 16) + 
    theme(plot.caption = element_text(size = 11))

p1.parking.2017

```


### Break down into time series components
We are often asked to interpret trends in data. Common questions include: 

* Is there seasonality in this data? If yes, can we adjust for the seasonality? 
* Is there a trend over the last X years? 
* Is this data unusual, or is it within line with previous trends? 

To do this, it's very useful to be decompose the time series into trend, seasonality, and remainder components. 

First we have to convert the counts into a time series object: 
```{r}
# reference: https://robjhyndman.com/hyndsight/dailydata/

t1.tickets.time.series <- df3.daily.tickets.joined %>% 
    pull(count) %>%   # pull out the count column as a vector
    ts(frequency = 365, 
       start = c(2016)) 

str(t1.tickets.time.series) 
```


Now we can run an  STL decomposition. 

```{r}
stl(t1.tickets.time.series,
    s.window = "periodic") %>% plot

# get decomposition as a df: 
df5.stl.decomp <- stl.as.df_fn(t1.tickets.time.series)

# join back on orig data: 
df5.stl.decomp <- cbind(df3.daily.tickets.joined, 
                        df5.stl.decomp) %>% 
    select(-"data")

# str(df4.stl.decomp)
# summary(df4.stl.decomp)
head(df5.stl.decomp, 20) %>% 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")

```

Notice the distribution of the remainder. This is what's "left over" after your decomposition model has been fitted to the past data. You should expect that the errors your forecast makes in future will have a similar distribution, at best. 

It's a good thing that the remainders have a nice bell shape with mean very close to zero. 

```{r}
df5.stl.decomp$remainder %>% 
    hist(50, main = "Distribution of remainders from STL decomposition")

abline(v = mean(df5.stl.decomp$remainder), 
       col = "red")
```









### Optional: use prophet package

```{r}
# fit model: 
m1 <- prophet(df3.daily.tickets.joined %>% 
                  rename(ds = entry_date, 
                         y = count))

# forecast next up to 30th March 2018: 
df6.future <- make_future_dataframe(m1, periods = 365)

# make prediction: 
df7.prophet.fcast <- predict(m1, df6.future)  

df7.prophet.fcast %>%
    tail(31) %>%
    select(ds,
           yhat_lower,
           yhat,
           yhat_upper) %>%
    mutate(ds = as.character(ds) %>% as.Date) %>%

    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
    

# plot fcast:
p2.prophet.fcast <-
    plot(m1, df7.prophet.fcast) +
    labs(title = "Forecast of number of daily parking tickets in Vancouver for 2018",
         subtitle = "Validation set: 2018-01-01 to 2018-03-30 \nRMSE on validation set = 167 \n",
         caption = "\n\nData source: https://data.vancouver.ca/datacatalogue/parking-tickets.htm ") +
    theme_classic(base_size = 16); p2.prophet.fcast


# decompositions: 
prophet_plot_components(m1, df7.prophet.fcast)

```

Let's plot prophet's fcast with actual data as well. 

First we'll prep the data for plotting. 

```{r}
df8.1.all.dates <- 
    data.frame(dates = seq(as.Date("2016-01-01"),
                           as.Date("2018-12-31"),
                           by="1 day"))
    
    
df8.hist.and.fcast <- 
    df8.1.all.dates %>% 
    full_join(df3.daily.tickets.joined, 
              by = c("dates" = "entry_date")) %>% 
    rename(count2017 = count) %>% 
    
    # now add 2018 data: 
    full_join(df4.daily.tickets.2018, 
              by = c("dates" = "entry_date")) %>% 
    rename(count2018 = count) %>% 
    
    # apparently have to change to POSIXCT: 
    mutate(dates = as.POSIXct(dates)) %>% 
    
    # now join on prophet fcast: 
    full_join(df7.prophet.fcast %>% 
                  filter(ds >= "2018-01-01") %>% 
                  select(ds, 
                         yhat_lower, 
                         yhat, 
                         yhat_upper), 
              by = c("dates" = "ds"))  


str(df8.hist.and.fcast)
df8.hist.and.fcast[729:740, ]

```


Let's evaluate the forecast error: 
```{r}
df9.validate.fcast <- 
    df8.hist.and.fcast %>% 
    filter(dates >= "2018-01-01", 
           dates <= "2018-03-30") %>% 
    select(dates, 
           count2018, 
           yhat) %>% 
    mutate(error = (count2018 - yhat),
           sq.error = (count2018 - yhat)^2)

rmse <- sqrt(mean(df9.validate.fcast$sq.error))
print(paste0("RMSE = ", round(rmse,1)))

mae <- mean(abs(df9.validate.fcast$error))
print(paste0("MAE = ", round(mae, 1)))

```


Now the plot: 
```{r}
p3.fcast.validation <- 
    df8.hist.and.fcast %>% 
    filter(dates >= "2017-12-01",
           dates <= "2018-03-30") %>% 
    ggplot(aes(x = dates, 
               y = count2017)) + 
    geom_line() + 
    
    # add 2018 actuals: 
    geom_line(data = df8.hist.and.fcast %>%
                  filter(dates >= "2017-12-01",
                         dates <= "2018-03-30"), 
              aes(x = dates, 
                  y = count2018), 
              colour = "blue", 
              size = 1) + 
    
    # add 2018 fcast: 
    geom_line(data = df8.hist.and.fcast %>%
                  filter(dates >= "2017-12-01",
                         dates <= "2018-03-30"), 
              aes(x = dates, 
                  y = yhat), 
              colour = "steelblue") + 
    
    # add confidence intervals: 
    geom_ribbon(data = df8.hist.and.fcast %>%
                  filter(dates >= "2017-12-01",
                         dates <= "2018-03-30"),
                aes(ymin = yhat_lower, 
                    ymax = yhat_upper), 
                fill = "grey60", 
                alpha = .2) + 
    
    labs(title = "Evaluating forecast of number of parking tickets in 2018", 
         subtitle = paste0("Training data: 2016-01-01 to 2017-12-31 \nValidation data: 2018-01-01 to 2018-03-30 \nRMSE = ", round(rmse), "\n")) + 
    theme_classic(base_size = 16)

p3.fcast.validation


```















## Write outputs 
```{r}
ggsave(here::here("results", 
                  "output from src", 
                  "2018-11-03_forecast-parking-tickets-full-2018.pdf"), 
       p2.prophet.fcast, 
       width = 10)

ggsave(here::here("results", 
                  "output from src", 
                  "2018-11-03_validating-2018-fcast.pdf"), 
       p3.fcast.validation, 
       width = 10)

```

