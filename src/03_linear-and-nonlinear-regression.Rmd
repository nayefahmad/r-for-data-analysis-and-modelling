---
title: "Day 3 - Linear and nonlinear regression"
author: "Nayef Ahmad - VCH Decision Support"
date: "November 16, 2018"
output: 
    html_document: 
        toc: yes
        code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)

library("kableExtra")


# shortcuts: 
# > ctrl + alt + i ==> new code chunk
# > ctrl + shift + k ==> knit doc 
# \  ==> line break (include 2 spaces after)
```


## Load packages for today's session 

```{r}
library(dplyr)  # for manipulating data frames 
library(ggplot2)  # for data viz
library(here)  # for simplifying folder references 
library(readr)  # reading and writing data files 
library(GGally)  # extension to ggplot2
library(ISLR)  # from the book Intro to Stat Learning with R 
library(broom)  # for saving model outpus as dataframes 

```

## Motivation for regression models 

Have you ever been asked to ...

* Calculate an average of a variable? *(e.g. ALOS? Avg daily ED visits?)*
* Investigate whether the average of one variable is related to one or more other variables? *(e.g. ALOS by nursing unit? ED visits by time of day?)* 
* Use historical/sample data to make inferences about a population or what might happen in the future? 

If you answered yes to all, regression models may be right for you. 

**A regression models is a quantitative theory of how the mean/expected value of one variable (Y) changes as the values of other variables (X1, x2, ...) change.**

Remember, all models are wrong but some are useful.^[https://en.wikipedia.org/wiki/All_models_are_wrong] Calculating an average was considered cutting-edge modelling in the 16th century.^[https://en.wikipedia.org/wiki/Average#Origin] In a sense, the average is "wrong" because no single data point might actually take the average value.^[https://www.gse.harvard.edu/news/ed/15/08/beyond-average]  

Similarly, regression models simplify away a lot of detail from the raw data. This is a feature, not a bug. 

## Steps in regression modelling 

1) Fit a model
2) Diagnostics: any problems with the model? Is it suitable for the data? 
3) Check overall significance and significance of individual predictors 
4) Check goodness of fit 
5) Fit another model; compare new model with old one, then go with the one that's better (there are algorithmic approaches for making these decisions)


## mtcars example 
Remember this graph? 

```{r}
p1.group.means <- mtcars %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = mpg)) + 
    
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # examine three different ways of summarizing behaviour within
    # each level of cyl: 
    
    # mean by group: 
    stat_smooth(aes(group = cyl,
                    colour = cyl), 
                method = "lm", 
                formula = y ~ 1) + 
    
    theme_classic()
    
# print: 
p1.group.means

```

Two ways to get those horizontal lines: 

### Averages by group: 
```{r}
mtcars %>% 
    group_by(cyl) %>% 
    summarize(mean.mpg = mean(mpg)) %>% 
    
    # don's use these lines; they're only for RMarkdown docs: 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
    
    
```


\  
\  
\  

### Regression on categorical variable: 
We use the ```lm``` function to fit a regression model. ```summary``` can be used to examine the model. 

```{r}
df1.mtcars <- mtcars %>% 
    mutate(cyl = as.factor(cyl))

m1.mpg.vs.cyl <- lm(mpg ~ cyl - 1, data = df1.mtcars)

summary(m1.mpg.vs.cyl)


```

The default output is hard to save, so we'll use the ```broom``` package to clean up and create a nice dataframe. 

```{r}
#1 row per coefficient 
broom::tidy(m1.mpg.vs.cyl)  

# 1 row per model; very useful when comparing lots of models
broom::glance(m1.mpg.vs.cyl)  

# 1 row per observation
broom::augment(m1.mpg.vs.cyl) %>% head


```

```{r}
write_csv(broom::tidy(m1.mpg.vs.cyl), 
          here("results", 
               "output from src", 
               "mtcars-regression.csv"))
```

\  
\  
\  


### Interpreting the change in the mean mpg
In both cases above, we see that there's this negative trend: as num cylinders increases, mpg decreases. So what do we gain by doing the regression instead of just calculating averages?

1) Are these "trends" real or are they the result of chance? E.g. is the difference in sample means of 6- and 8- cylinder cars representative of a real difference in the population?

2) Will these "trends" still exist after we account for other variables? We shouldn't just filter out rows based on other variables - we should account for them systematically. 

3) What is the size of the trend (for continuous variables)? Can we compare the trends of two different sites? Can we detect changes in the size of the trend? Can we use the trend to fill in gaps in our knowledge (interpolation and extrapolation)? 


### Averages by group, after accounting for weight
Let's fit another regression, this time incuding weight as a predictor variable. 

```{r}
m2.include.wt <- lm(mpg ~ cyl + wt, data = df1.mtcars)

summary(m2.include.wt)
```

Note that the expected difference in mpg between 4-cylinder and 6-cylinder cars is **4.2556 mpg**, while it was `r 26.6636 - 19.7429` mpg previously. 

Similarly, after accounting for weight, the difference between 6-cylinder cars and 8-cylinder cars is **`r 6.0709-4.2556`** instead of `r 19.7429 - 15.1000`

**This is important. Our conclusions may be biased or irrelevant if we don't systematically control for confounding variables.** E.g. an observed difference in ALOS between two nursing units is almost certainly not due the characteristics of the two nursing units alone - there will be multiple confounding variables such as age, diagnoses, etc.  


#### Diagnostic plots 
Residuals from regression should be white noise with mean zero. This tells you that you have captured all the *systematic* structure in the relationship between the response and the predictor variables, leaving only *randomness/noise*. See [here](http://data.library.virginia.edu/diagnostic-plots/) for more details on interpreting these plots. 


```{r}
# display 4 plots in a 2 by 2 grid: 
par(mfrow = c(2, 2))

plot(m2.include.wt)


```

\  
\  
\  


### Using ```predict``` to interpret models 
Once you fit a model, it exists independently of the data that you started with. This is a powerful abstraction - instead of carrying around raw data to describe a system, you can have a simple mathematical model that is easier to interpret and work with. 

Although you will usually start by looking at regression coefficients, one of the best ways to interpet your model is to kick it and see what it does. In this context, kicking it refers to feeding it an artificial dataset that you create. 

First, just try calling ```predict```: 

```{r}
predict(m2.include.wt)
```

The result is a vector of numbers that gives you the predicted value for each row of the ```mtcars``` dataset. The ```broom::augment``` function is a good way to get these predicted values along with the input data. In the output below, ```.fitted``` gives the value that the regression model predicts, given values of cyl and wt. 

```{r}
augment(m2.include.wt) %>% 
    select(mpg, cyl, wt, .fitted) %>% 
    head(15) %>% 
    
    # don's use these lines; they're only for RMarkdown docs: 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
    
```


Let's see how the model uses ```wt``` and ```cyl``` values to predict mpg. We expect three parallel lines, one for each value of cyl, with slope equal to the coefficient of ```wt```. 


```{r}
df1.mtcars %>% 
    mutate(pred.mpg = predict(m2.include.wt)) %>% 
    
    ggplot(aes(x = wt, 
               y = pred.mpg)) + 
    
    geom_point(aes(col = cyl))

```

Yup, just what we expected. Now let's say you have some new data and you want to see what the model predicts for this new data. 

```{r}
# create new fake data: 

df2.mtcars.fake <- data.frame(cyl = as.factor(c(4, 6, 8)), 
                              wt = c(4, 2, 2))

df2.mtcars.fake %>% 
    
    # don's use these lines; they're only for RMarkdown docs: 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")


```

Now pass the fake data to ```predict```: 

```{r}
# note the "newdata" argument in predict() below

df2.mtcars.fake <- 
    df2.mtcars.fake %>% 
    mutate(pred.mpg = predict(m2.include.wt, 
                              newdata = df2.mtcars.fake)) 

# first plot the points that were originally in the data:  
df1.mtcars %>% 
    mutate(pred.mpg = predict(m2.include.wt)) %>% 
    
    ggplot(aes(x = wt, 
               y = pred.mpg)) + 
    
    geom_point(aes(col = cyl)) + 
    
    # then add the new points. Increase size to make them distinguishable: 
    geom_point(data = df2.mtcars.fake, 
               aes(x = wt, 
                   y = pred.mpg, 
                   col = cyl), 
               size = 5)


```


### Plotting actuals vs predicted values to assess model performance

Plotting the actual values of your response variable vs the predicted values from the model is a great way to see how your model is doing and to compare different models. In the best case scenario, all the points below should fall on the blue line with slope 1 and intercept 0.  

```{r}
augment(m2.include.wt) %>% 
    
    ggplot(aes(x = .fitted, 
               y = mpg)) + 
    
    geom_point(aes(col = cyl)) + 
    
    # add 45 degree line: 
    geom_abline(slope = 1, 
                intercept = 0, 
                col = "blue") + 
    
    # make sure axes have same scale: 
    scale_x_continuous(limits = c(10, 35)) + 
    scale_y_continuous(limits = c(10, 35)) + 
    
    labs(title = "Actuals vs predictions", 
         subtitle = "Points above the line are being under-estimated \nPoints below the line are being over-estimated")
    

```

TODO: WE HAVE TO MOVE AWAY FROM RE-DOING AD HOCS EVERY TIME, INGESTING DATA, SPITTING OUT RESULTS, BUT NEVER LEARNING ANYTHING. WE NEED MODELS OF IMPORTANT PROCESSES. 







***********************************************

\  
\  
\  
 
## Mini-Exercise: GDP of US Metro areas 

* Use the ```us-msa-gdp-and-population.csv``` dataset to find the relationship between GDP (in millions of USD) and population for US metropolitan statistical areas (MSAs). 
* How can you tell whether a particular MSA is under-performing or over-performing, relative to its population size? 
* Identify the top 3 over-performing MSAs. 


***********************************************

\  
\  
\  


## Transforamtions of variables and interactions between variables 

```{r}
df2.advert <- read_csv(here::here("data", 
                                  "Advertising.csv")) %>% 
    select(-X1)

str(df2.advert)
summary(df2.advert)
head(df2.advert)

```



## Boston house prices dataset 

```{r}

```






