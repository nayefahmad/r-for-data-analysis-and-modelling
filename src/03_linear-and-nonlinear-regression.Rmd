---
title: "Day 3 - Linear and nonlinear regression"
author: "Nayef Ahmad - VCH Decision Support"
date: "November 16, 2018"
output: 
    html_document: 
        toc: yes
        code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE)

library("kableExtra")


# shortcuts: 
# > ctrl + alt + i ==> new code chunk
# > ctrl + shift + k ==> knit doc 
# \  ==> line break (include 2 spaces after)
```


## Load packages for today's session 

```{r}
library(dplyr)  # for manipulating data frames 
library(ggplot2)  # for data viz
library(here)  # for simplifying folder references 
library(readr)  # reading and writing data files 
library(GGally)  # extension to ggplot2
library(ISLR)  # from the book Intro to Stat Learning with R 
library(broom)  # for saving model outpus as dataframes 

```

## Motivation for regression models 

Have you ever been asked to ...

* Calculate an average of a variable? *(e.g. ALOS? Avg daily ED visits?)*
* Investigate whether the average of one variable is related to one or more other variables? *(e.g. ALOS by nursing unit? ED visits by time of day?)* 
* Use historical/sample data to make inferences about a population or what might happen in the future? 

If you answered yes to all, regression models may be right for you. 

**A regression models is a quantitative theory of how the mean/expected value of one variable (Y) changes as the values of other variables (X1, x2, ...) change.**

Remember, all models are wrong but some are useful.^[https://en.wikipedia.org/wiki/All_models_are_wrong] Calculating an average was considered cutting-edge modelling in the 16th century.^[https://en.wikipedia.org/wiki/Average#Origin] In a sense, the average is "wrong" because no single data point might actually take the average value.^[https://www.gse.harvard.edu/news/ed/15/08/beyond-average]  

Similarly, regression models simplify away a lot of detail from the raw data. This is a feature, not a bug. 


## mtcars example 
Remember this graph? 

```{r}
p1.group.means <- mtcars %>%
    
    # let's recode cyl as a discrete variable: 
    mutate(cyl = factor(cyl, 
                        levels = c(4, 6, 8))) %>% 
    
    # now start using gpplot functions: 
    ggplot(aes(x = disp,  # specify x and y axis
               y = mpg)) + 
    
    geom_point(aes(colour = cyl, 
                   size = hp)) + 
    
    # examine three different ways of summarizing behaviour within
    # each level of cyl: 
    
    # mean by group: 
    stat_smooth(aes(group = cyl,
                    colour = cyl), 
                method = "lm", 
                formula = y ~ 1) + 
    
    theme_classic()
    
# print: 
p1.group.means

```

Two ways to get those horizontal lines: 

### Averages by group: 
```{r}
mtcars %>% 
    group_by(cyl) %>% 
    summarize(mean.mpg = mean(mpg)) %>% 
    
    # don's use these lines; they're only for RMarkdown docs: 
    kable() %>% 
    kable_styling(bootstrap_options = c("striped",
                                        "condensed", 
                                        "responsive"), 
                  full_width = FALSE, 
                  position = "left")
    
    
```


\  
\  
\  

### Regression on categorical variable: 
We use the ```lm``` function to fit a regression model. ```summary``` can be used to examine the model. 

```{r}
df1.mtcars <- mtcars %>% 
    mutate(cyl = as.factor(cyl))

m1.mpg.vs.cyl <- lm(mpg ~ cyl - 1, data = df1.mtcars)

summary(m1.mpg.vs.cyl)


```

The default output is hard to save, so we'll use the ```broom``` package to clean up and create a nice dataframe. 

```{r}
broom::tidy(m1.mpg.vs.cyl)  
#1 row per coefficient 

broom::glance(m1.mpg.vs.cyl)  
# 1 row per model; very useful when comparing lots of models

broom::augment(m1.mpg.vs.cyl) %>% head
# 1 row per observation

```

```{r}
write_csv(broom::tidy(m1.mpg.vs.cyl), 
          here("results", 
               "output from src", 
               "mtcars-regression.csv"))
```




### Interpreting the change in the mean mpg
In both cases above, we see that there's this negative trend: as num cylinders increases, mpg decreases. So what do we gain by doing the regression instead of just calculating averages?

1) Are these "trends" real or are they the result of chance? E.g. is the difference in sample means of 6- and 8- cylinder cars representative of a real difference in the population?

2) Will these "trends" still exist after we account for other variables? We shouldn't just filter out rows based on other variables - we should account for them systematically. 

3) What is the size of the trend (for continuous variables)? Can we compare the trends of two different sites? Can we detect changes in the size of the trend? Can we use the trend to fill in gaps in our knowledge (interpolation and extrapolation)? 


### Averages by group, after accounting for weight
Let's fit another regression, this time incuding weight as a predictor variable. 

```{r}
m2.include.wt <- lm(mpg ~ cyl + wt, data = df1.mtcars)

summary(m2.include.wt)
```

Note that the expected difference in mpg between 4-cylinder and 6-cylinder cars is **4.2556 mpg**, while it was `r 26.6636 - 19.7429` mpg previously. 

Similarly, after accounting for weight, the difference between 6-cylinder cars and 8-cylinder cars is **`r 6.0709-4.2556`** instead of `r 19.7429 - 15.1000`

**This is important. Our conclusions may be biased or irrelevant if we don't systematically control for confounding variables.** E.g. an observed difference in ALOS between two nursing units is almost certainly not due the characteristics of the two nursing units alone - there will be multiple confounding variables such as age, diagnoses, etc.  

***********************************************

\  
\  
\  
 

## Steps in regression modelling 

1) Fit a model
2) Diagnostics: any problems with the model? Is it suitable for the data? 
3) Check overall significance and significance of individual predictors 
4) Check goodness of fit 
5) Fit another model; compare new model with old one, then go with the one that's better (there are algorithmic approaches for making these decisions)

We'll look at the Advertising dataset from ISLR. 

```{r}
df2.advert <- read_csv(here::here("data", 
                                  "Advertising.csv")) %>% 
    select(-X1)

str(df2.advert)
summary(df2.advert)
head(df2.advert)

```



### 

```{r}

```






